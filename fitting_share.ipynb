{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c567b44-2856-407c-ac7c-53273036bed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49063c5-b661-48fb-91de-e195dcb21a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions with 4 parameters\n",
    "def sigmoid4(x,a,b,c,d):           \n",
    "    return a+b/(1+np.exp(c*(x+d)))\n",
    "def arctan4(x,a,b,c,d):   \n",
    "    return a +(2/np.pi)*b * np.arctan(c*(x+d))\n",
    "def erf4(x,a,b,c,d):     \n",
    "    return a+b*sp.special.erf(c*(x+d)) \n",
    "def tanhyp4(x,a,b,c,d):     #This is mathematically the same as a logistic (sigmoid) function!. Use the result for testing\n",
    "    return a+b*np.tanh(c*(x+d))\n",
    "\n",
    "funcs4 = [sigmoid4,arctan4,erf4,tanhyp4]\n",
    "vec_funcs4=[*map(np.vectorize,funcs4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b5a1d-0436-478a-a820-6bed5c17d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial parameter guesses for fits. The fitting routine can get stuck in local minima.\n",
    "#trial and error is required to find initial parameters that lead to good fits.\n",
    "#rough initial parameters can be estimated by physical considerations: e.g. start at 0.5 AUC, increase with training size, etc.\n",
    "#columns are: trait,left asymptotic value,inflection pt,right asymptotic guess, slope at inflection point \n",
    "inits= pd.DataFrame([\n",
    "[\"asthma\",.5,4,.65,2], #done\n",
    " [\"atrial.fibrillation\",.5,3,.65,2], \n",
    " ['breast',.5,3,.65,2], \n",
    " ['breast-lou',.5,3,.65,2], \n",
    " [\"diabetes.type2\",.5,3,.65,2], \n",
    " [\"diabetes.type1\",.5,3,.65,2], \n",
    " ['CAD',.5,3,.65,2], \n",
    " [\"hypertension\",0.5, 4.5,.65,3], \n",
    " [\"hyperlip-cont\",0.5, 4.5,.65,3], \n",
    " ['gout',0.5,3.5,.7,2],\n",
    " [\"Direct.bilirubin\",.05,3,.45,2],\n",
    " ['BMI',.05,3,.45,2],\n",
    " ['hgt',.05,3,.55,2],\n",
    " ['Lipoprotein.A',.15,3,.65,2]])\n",
    "inits.columns=['trait','left_asymp','inflection_pt','asymp_guess','slope']\n",
    "inits=inits.set_index('trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e44d37-beda-458d-9ce1-b0ab7d016872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define paramter bounds. These serve two purposes:\n",
    "#1) parameters are kept within physical parameter ranges, e.g.,\n",
    "#AUC<1, correlation<1, metrics grow with training size, etc.\n",
    "#2) they stop the fitting routine if a parameter is running off to \n",
    "#infinity and the computation is taking a long time because of that.\n",
    "\n",
    "#Fitting routine can be run without any bounds by simply replacing\n",
    "#these arrays with empty arrays\n",
    "\n",
    "#CACO\n",
    "caco_bnds = [((0.3,0,-50,-10),(0.65,0.5,5,-0.5)),          \n",
    "             ((0.3,-.5,-50,-10),(.7,0.5,5,-0.5)),\n",
    "             ((0.3,-.5,-50,-10),(.7,0.5,5,-0.5)),\n",
    "             ((0.3,-.5,-50,-10),(.7,0.5,5,-0.5)),\n",
    "             ((0.3,0,0,-10),(0.65,0.4,5,-0.5))]\n",
    "#CONT\n",
    "cont_bnds = [((-0.1,0,-50,-10),(0.65,0.75,5,-0.5)),          \n",
    "             ((-0.1,0,-50,-10),(.7,0.5,5,-0.5)),\n",
    "             ((-0.1,0,-50,-10),(.7,0.5,5,-0.5)),         \n",
    "             ((-0.1,0,-50,-10),(.7,0.5,5,-0.5)),\n",
    "             ((-0.1,0,0,-10),(0.65,0.4,5,-0.5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ebb87-7561-4869-be92-7ed2e2015453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define fitting and analysis functions that are used below\n",
    "\n",
    "def fitsingle(xs,ys,yerrs,func,init,**kwargs):\n",
    "    opt_params={'bounds' : None, 'secret':13}\n",
    "    for key, value in kwargs.items():\n",
    "        if key in opt_params.keys():\n",
    "            opt_params[key]=value\n",
    "    if opt_params['bounds'] == None:\n",
    "        return sp.optimize.curve_fit(func,xs,ys,p0=init,sigma=yerrs,absolute_sigma=True,maxfev = 4000)\n",
    "    else:\n",
    "        return sp.optimize.curve_fit(func,xs,ys,p0=init,sigma=yerrs,absolute_sigma=True,maxfev = 4000,bounds=opt_params['bounds'])\n",
    "\n",
    "#func that gives chi2 function\n",
    "def chisq(xs,ys,yerrs,result,numofparams,funcs):\n",
    "    ind=0\n",
    "    expect=[]\n",
    "    for func in funcs: \n",
    "        expect.append(func(xs,*result[ind][0]))\n",
    "        ind += 1\n",
    "    chisq =np.array([*map(np.sum,((ys-expect)/yerrs)**2)])\n",
    "    df = len(xs)-4\n",
    "    chisqperdof = np.array([x / df for x in chisq])\n",
    "    return chisq, df, chisqperdof\n",
    "\n",
    "#func to build cholesky matrix\n",
    "def cholesky(result,numofparams):\n",
    "    numofmodels = len(result)\n",
    "    #central values\n",
    "    centers = np.empty([numofmodels,numofparams])\n",
    "    for i in range(0,numofmodels):\n",
    "        centers[i] = result[i][0]\n",
    "    #param errors\n",
    "    errs = np.empty([numofmodels,numofparams])\n",
    "    for i in range(0,numofmodels):\n",
    "        errs[i]=np.sqrt(np.diag(result[i][1]))\n",
    "    #convert covariance to correlation matrix\n",
    "    corr = np.empty([numofparams*numofmodels,numofparams])\n",
    "    for i in range(0,numofmodels):\n",
    "        temp=1/np.sqrt(np.diag(result[i][1]))\n",
    "        corr[i*numofparams:(i*numofparams+numofparams)]=np.dot(np.dot(np.diagflat(temp),result[i][1]),np.diagflat(temp))\n",
    "    #do cholesky decomp\n",
    "    chol = np.empty([numofparams*numofmodels,numofparams])\n",
    "    for i in range(0,numofmodels):\n",
    "        chol[i*numofparams:(i*numofparams+numofparams)]=np.linalg.cholesky(corr[i*numofparams:(i*numofparams+numofparams)])\n",
    "    return chol\n",
    "\n",
    "#MCpts builds monte carlo data using the cholesky method while allowing parameters to vary as correlated gaussian normal distributions.\n",
    "def MCpts(numofpts,numofmodels,numofparams,cholesky,result):\n",
    "    centers = np.empty([numofmodels,numofparams])\n",
    "    for i in range(0,numofmodels):\n",
    "        centers[i] = result[i][0]\n",
    "    errs = np.empty([numofmodels,numofparams])\n",
    "    for i in range(0,numofmodels):\n",
    "        errs[i]=np.sqrt(np.diag(result[i][1]))\n",
    "    mcptstotal = numofpts\n",
    "    mcpts=np.empty([numofmodels*mcptstotal,numofparams])\n",
    "    for i in range(0,numofmodels):\n",
    "        for j in range(0,mcptstotal):\n",
    "            mcpts[i*mcptstotal+j] = centers[i]+np.multiply(np.dot( cholesky[i*numofparams:(i*numofparams+numofparams)],np.random.normal(loc=0.0, scale=1.0, size=(numofparams,1))).flatten(),errs[i])\n",
    "    return mcpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e904094-dc5b-4520-95c1-91085df3536b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m cohortPATH\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath to cohort files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#load sibling IDs. Sibling file is two columns of IDs where the columns represent sibling pairs. Each sibling pair appears only once. \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#NEED TO SET\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m sibs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath to list of sibling IDs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#covariates adjusted for: commonly PCA (PC vectors), year of birth (YOB), and sex\u001b[39;00m\n\u001b[1;32m     21\u001b[0m covTYPE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCA_YOB_SEX\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "trait=['TRAIT NAME']\n",
    "\n",
    "#Path to top working directory\n",
    "topPATH = 'Path to working directory'\n",
    "#Phenotype style: CACO (case control) or CONT\n",
    "phenTYPE = 'CACO'\n",
    "#path to different cohort group files, e.g., list of IDs of individuals who are siblings or a specific ancestry\n",
    "cohortPATH='Path to cohort files'\n",
    "#load sibling IDs. Sibling file is two columns of IDs where the columns represent sibling pairs. Each sibling pair appears only once. \n",
    "#NEED TO SET\n",
    "sibs = pd.read_csv('Path to list of sibling IDs')\n",
    "#covariates adjusted for: commonly PCA (PC vectors), year of birth (YOB), and sex\n",
    "covTYPE = 'PCA_YOB_SEX'\n",
    "#number of monte carlo samples  for fitting\n",
    "n_mc=5000    \n",
    "#Load list of train sizes #NEED TO SET\n",
    "trainsizes=np.loadtxt('Path to list of training sizes used')\n",
    "#load prediction metrics (e.g., AUC or correlation)\n",
    "#Note there can be inherent uncertainty in how you choose the maximal value (i.e. hyperparameter) in the validation set.\n",
    "#In the manuscript, the sib_met is selected by taking the hyperparameter for the maximal value in the \n",
    "#validation set and then using that hyperparameter to score the siblings or ancestry group.\n",
    "metrics=np.empty((len(trainsizes),5))\n",
    "for m in trainsizes:\n",
    "    for i in range(1,6):\n",
    "        sib_met = np.loadtxt('Path to metrics for siblings with files index by m value and CV-fold i')\n",
    "        metrics[np.where(trainsizes==m),i-1] = sib_met          \n",
    "pheno=pd.read_csv('Path to phenotype file with columns: individual ID, family ID, adjusted phenotype value')\n",
    "#generate fitting data: train sizes vs mean (over CV-folds) metric value\n",
    "xs=trainsizes\n",
    "ys=np.mean(metrics,axis=1)\n",
    "if gwasTYPE == 'CACO':\n",
    "    #load the case control status\n",
    "    caco = pd.read_csv('Path to full case control status values')\n",
    "    #Subset down the full case control list to just siblings\n",
    "    sib_caco = caco[caco[0].isin(sibs[0]) | caco[0].isin(sibs[1])]\n",
    "    #total number of cases\n",
    "    sibcas=sib_caco[2].sum()\n",
    "    #total uncertainty on the average AUC value (computation explained in the manuscript). The first term comes from running\n",
    "    #multiple CVs and the second term comes from the uncertainty of computing AUC with finite statistics \n",
    "    yerrs=np.sqrt(np.std(metrics,axis=1)**2+(1/(3*np.sqrt(sibcas)))**2)\n",
    "else:\n",
    "    #subset the full phenotype file down to just siblings\n",
    "    sib_pheno=pheno[pheno[0].isin(sibs[0]) | pheno[0].isin(sibs[1])]\n",
    "    #total uncertainty on the average correlation value (computation explained in the manuscript). The first term comes from running\n",
    "    #multiple CVs and the second term comes from the uncertainty of computing correlation with finite statistics \n",
    "    yerrs=np.sqrt(np.std(metrics,axis=1)**2+(1-ys**2)/(len(sib_pheno)-2))\n",
    "    \n",
    "    \n",
    "#initial guesses and make fits\n",
    "#call values from tables\n",
    "if trait in inits.index:\n",
    "    left_asymp = inits.loc[trait]['left_asymp']; inflection_pt = inits.loc[trait]['inflection_pt']; \n",
    "    asymp_guess = inits.loc[trait]['asymp_guess']; slope = inits.loc[trait]['slope']\n",
    "else: #if no values exist in table\n",
    "    left_asymp = 0.5; inflection_pt = 3; asymp_guess = .65; slope = 2;\n",
    "#convert initial guesses to parameter values for the various functions used\n",
    "params4=[\n",
    "     [left_asymp,asymp_guess-left_asymp,-slope,-inflection_pt],\n",
    "     [asymp_guess/2,asymp_guess/2,slope,-inflection_pt],\n",
    "     [asymp_guess/2,asymp_guess/2,slope,-inflection_pt],\n",
    "     [asymp_guess/2,asymp_guess/2,slope,-inflection_pt]]\n",
    "#load parameter bounds if using them\n",
    "if gwasTYPE != 'CACO':\n",
    "    bnds=cont_bnds\n",
    "else:\n",
    "    bnds=caco_bnds\n",
    "\n",
    "####################################\n",
    "####################################\n",
    "### FITTING ROUTINE AND ANALYSIS ###\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "#run fitting routing\n",
    "result4=[]\n",
    "for i in range(0,len(funcs4)):\n",
    "    if bnds[i]==None:\n",
    "        result4.append(fitsingle(np.log10(xs),ys,yerrs,funcs4[i],params4[i])    )\n",
    "    else:\n",
    "        result4.append(fitsingle(np.log10(xs),ys,yerrs,funcs4[i],params4[i],bounds=bnds[i]))\n",
    "#get chisq\n",
    "chisq4 = chisq(np.log10(xs),ys,yerrs,result4,4,funcs4)\n",
    "#build cholesky matrices\n",
    "cho4 = cholesky(result4,4)\n",
    "#build MC points\n",
    "mcpts4 = MCpts(n_mc,len(funcs4),4,cho4,result4)\n",
    "#filter out \"unphysical\" MC pts. in this case we are only using the condition b>0\n",
    "altpts=[]\n",
    "altlens=[]\n",
    "for i in range(0,len(funcs4)):\n",
    "    tmp=pd.DataFrame(mcpts4[i*n_mc:(i+1)*n_mc])\n",
    "    altpts.append(tmp[(tmp[1]>0)&((tmp[0]+tmp[1])<1)].to_numpy())\n",
    "    altlens.append(tmp[(tmp[1]>0)&((tmp[0]+tmp[1])<1)].shape[0])\n",
    "altfirst = np.insert(np.cumsum(altlens),0,0)\n",
    "\n",
    "#define asymptotic values as an average over MC asymptotes (THIS DEPENDS ON SPECIFIC FORM OF THE FUNCTIONS)  \n",
    "asymp4 = np.empty([len(funcs4),2])\n",
    "for i in range(0,len(funcs4)):\n",
    "    temp = altpts[i][:,0]+altpts[i][:,1]\n",
    "    asymp4[i,0]=np.mean(temp)\n",
    "    asymp4[i,1]=np.std(temp)\n",
    "        \n",
    "\n",
    "#generate band data\n",
    "nxs=500\n",
    "tmpxs = np.linspace(0,8,int(nxs/2))\n",
    "tmpxs = np.hstack((tmpxs,np.random.normal(loc=-result4[0][0][3],scale=1,size=int(nxs/2))))\n",
    "tmpxs = np.sort(tmpxs)\n",
    "#minmax4 will be the list of minimum and maximum values for the uncertainty bands generated with MC\n",
    "#minmax4 will have 4 sets of minmax data, one for each function\n",
    "minmax4=[]\n",
    "df_asymp4 = np.empty([len(funcs4),2])\n",
    "for func in funcs4:\n",
    "    xs=np.log10(trainsizes)\n",
    "    dof=len(xs)-4\n",
    "    cs=[]\n",
    "    for i in range(0,altlens[funcs4.index(func)]):\n",
    "        cs.append(np.sum(((ys-func(xs,*altpts[funcs4.index(func)][i]))/yerrs)**2))\n",
    "    df=pd.DataFrame(cs)\n",
    "\n",
    "    # num of sigmas in each direction\n",
    "    # 1 sigma = 1-0.682 = 0.318\n",
    "    # 2 sigma = 1-0.954 = 0.046\n",
    "    # 3 sigma = 1-0.996 = 0.004\n",
    "    band_sigs = {'1sig': 0.318, '2sig':0.046,'3sig':0.004}\n",
    "    cl = band_sigs[band_sig]\n",
    "    #one sided cut\n",
    "    cl_cut=df.quantile(q=1-cl).values[0]\n",
    "    #indices with data within the confidence limit\n",
    "    inds=df[df[0]<cl_cut].index.values\n",
    "\n",
    "    tmp_minmax4=np.zeros((nxs,2))\n",
    "    for j in range(0,nxs):\n",
    "        tmp=[]\n",
    "        for i in inds:\n",
    "            tmp.append(func(tmpxs[j],*altpts[funcs4.index(func)][i]))\n",
    "        #be careful of poor fit outliers\n",
    "        tmp_minmax4[j,:] = [np.min(tmp),np.max(tmp)]\n",
    "    minmax4.append(tmp_minmax4)\n",
    "    \n",
    "    #df_asymp4 is a dataframe with the (right side) asymptotic results for each function\n",
    "    #averaged over all MC points in the confidence interval.\n",
    "    tmp_asymp=altpts[funcs4.index(func)][inds].T\n",
    "    df_asymp4[funcs4.index(func),0]=np.mean(tmp_asymp[0] + tmp_asymp[1])\n",
    "    df_asymp4[funcs4.index(func),1]=np.std(tmp_asymp[0] + tmp_asymp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3b6bd-2876-488b-8919-6bb840d59466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
